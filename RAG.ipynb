{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23521,
     "status": "ok",
     "timestamp": 1737557682432,
     "user": {
      "displayName": "chaimae hadrouch",
      "userId": "11701385554007709212"
     },
     "user_tz": -60
    },
    "id": "6lzun33QZATK",
    "outputId": "be8fd304-b179-4732-f918-0fae5539fbd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting adapters\n",
      "  Downloading adapters-1.0.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.27.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud) (3.10.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Downloading faiss_cpu-1.9.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Downloading adapters-1.0.1-py3-none-any.whl (283 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, rank_bm25, PyPDF2, fsspec, faiss-cpu, dill, multiprocess, tokenizers, transformers, datasets, adapters\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.1\n",
      "    Uninstalling transformers-4.47.1:\n",
      "      Successfully uninstalled transformers-4.47.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyPDF2-3.0.1 adapters-1.0.1 datasets-3.2.0 dill-0.3.8 faiss-cpu-1.9.0.post1 fsspec-2024.9.0 multiprocess-0.70.16 rank_bm25-0.2.2 tokenizers-0.20.3 transformers-4.45.2 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers faiss-cpu wordcloud transformers PyPDF2 datasets rank_bm25 adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8268,
     "status": "ok",
     "timestamp": 1737557695668,
     "user": {
      "displayName": "chaimae hadrouch",
      "userId": "11701385554007709212"
     },
     "user_tz": -60
    },
    "id": "WRqQRC_KZTc6",
    "outputId": "b477e16d-b34d-49ce-aef4-d9a702d716dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 16431,
     "status": "ok",
     "timestamp": 1737557754009,
     "user": {
      "displayName": "chaimae hadrouch",
      "userId": "11701385554007709212"
     },
     "user_tz": -60
    },
    "id": "hTCy4pIVYqNX"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "from pypdf import PdfReader\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from adapters import AdapterConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import re\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wz4Bbz5SbRa5"
   },
   "source": [
    "# Mount Your Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28731,
     "status": "ok",
     "timestamp": 1737557796581,
     "user": {
      "displayName": "chaimae hadrouch",
      "userId": "11701385554007709212"
     },
     "user_tz": -60
    },
    "id": "u1VGKeVJbS4i",
    "outputId": "c6b7858b-9a78-4217-8f96-53be516d5674"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrPbgFq4bhjE"
   },
   "source": [
    "# Create a Directory in Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1737557801899,
     "user": {
      "displayName": "chaimae hadrouch",
      "userId": "11701385554007709212"
     },
     "user_tz": -60
    },
    "id": "5LE2hDSBbjFq",
    "outputId": "a2aef45c-ebfc-42dd-fc80-e7242d09398c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created at: /content/drive/My Drive/RAG_Project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory path in your Google Drive\n",
    "directory_path = \"/content/drive/My Drive/RAG_Project\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "print(f\"Directory created at: {directory_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvWBC5S2Z3Fg"
   },
   "source": [
    "# Creation of a JSON file for the sections and pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1737557810866,
     "user": {
      "displayName": "chaimae hadrouch",
      "userId": "11701385554007709212"
     },
     "user_tz": -60
    },
    "id": "jiu61LIyZy6p"
   },
   "outputs": [],
   "source": [
    "def create_sections_json(output_file=\"/content/drive/MyDrive/RAG_Project/sections.json\"):\n",
    "    \"\"\"\n",
    "    Generates the sections.json file with the hierarchical structure of the book.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"Introduction to Psychology\": {\n",
    "            \"page_start\": 7,\n",
    "            \"page_end\": 34,\n",
    "            \"subsections\": {\n",
    "                \"What Is Psychology?\": {\n",
    "                    \"page_start\": 8,\n",
    "                    \"page_end\": 8\n",
    "                },\n",
    "                \"History of Psychology\": {\n",
    "                    \"page_start\": 9,\n",
    "                    \"page_end\": 17\n",
    "                },\n",
    "                \"Contemporary Psychology\": {\n",
    "                    \"page_start\": 18,\n",
    "                    \"page_end\": 25\n",
    "                },\n",
    "                \"Careers in Psychology\": {\n",
    "                    \"page_start\": 26,\n",
    "                    \"page_end\": 29\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Psychological Research\": {\n",
    "            \"page_start\": 35,\n",
    "            \"page_end\": 70,\n",
    "            \"subsections\": {\n",
    "                \"Why Is Research Important?\": {\n",
    "                    \"page_start\": 36,\n",
    "                    \"page_end\": 40\n",
    "                },\n",
    "                \"Approaches to Research\": {\n",
    "                    \"page_start\": 41,\n",
    "                    \"page_end\": 47\n",
    "                },\n",
    "                \"Analyzing Findings\": {\n",
    "                    \"page_start\": 48,\n",
    "                    \"page_end\": 58\n",
    "                },\n",
    "                \"Ethics\": {\n",
    "                    \"page_start\": 59,\n",
    "                    \"page_end\": 62\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Biopsychology\": {\n",
    "            \"page_start\": 71,\n",
    "            \"page_end\": 108,\n",
    "            \"subsections\": {\n",
    "                \"Human Genetics\": {\n",
    "                    \"page_start\": 72,\n",
    "                    \"page_end\": 77\n",
    "                },\n",
    "                \"Cells of the Nervous System\": {\n",
    "                    \"page_start\": 78,\n",
    "                    \"page_end\": 83\n",
    "                },\n",
    "                \"Parts of the Nervous System\": {\n",
    "                    \"page_start\": 84,\n",
    "                    \"page_end\": 85\n",
    "                },\n",
    "                \"The Brain and Spinal Cord\": {\n",
    "                    \"page_start\": 86,\n",
    "                    \"page_end\": 96\n",
    "                },\n",
    "                \"The Endocrine System\": {\n",
    "                    \"page_start\": 97,\n",
    "                    \"page_end\": 99\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"States of Consciousness\": {\n",
    "            \"page_start\": 109,\n",
    "            \"page_end\": 143,\n",
    "            \"subsections\": {\n",
    "                \"What Is Consciousness?\": {\n",
    "                    \"page_start\": 110,\n",
    "                    \"page_end\": 113\n",
    "                },\n",
    "                \"Sleep and Why We Sleep\": {\n",
    "                    \"page_start\": 114,\n",
    "                    \"page_end\": 116\n",
    "                },\n",
    "                \"Stages of Sleep\": {\n",
    "                    \"page_start\": 117,\n",
    "                    \"page_end\": 120\n",
    "                },\n",
    "                \"Sleep Problems and Disorders\": {\n",
    "                    \"page_start\": 121,\n",
    "                    \"page_end\": 125\n",
    "                },\n",
    "                \"Substance Use and Abuse\": {\n",
    "                    \"page_start\": 126,\n",
    "                    \"page_end\": 133\n",
    "                },\n",
    "                \"Other States of Consciousness\": {\n",
    "                    \"page_start\": 134,\n",
    "                    \"page_end\": 136\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Sensation and Perception\": {\n",
    "            \"page_start\": 145,\n",
    "            \"page_end\": 179,\n",
    "            \"subsections\": {\n",
    "                \"Sensation versus Perception\": {\n",
    "                    \"page_start\": 146,\n",
    "                    \"page_end\": 148\n",
    "                },\n",
    "                \"Waves and Wavelengths\": {\n",
    "                    \"page_start\": 149,\n",
    "                    \"page_end\": 152\n",
    "                },\n",
    "                \"Vision\": {\n",
    "                    \"page_start\": 153,\n",
    "                    \"page_end\": 160\n",
    "                },\n",
    "                \"Hearing\": {\n",
    "                    \"page_start\": 161,\n",
    "                    \"page_end\": 163\n",
    "                },\n",
    "                \"The Other Senses\": {\n",
    "                    \"page_start\": 164,\n",
    "                    \"page_end\": 167\n",
    "                },\n",
    "                \"Gestalt Principles of Perception\": {\n",
    "                    \"page_start\": 168,\n",
    "                    \"page_end\": 171\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Learning\": {\n",
    "            \"page_start\": 181,\n",
    "            \"page_end\": 211,\n",
    "            \"subsections\": {\n",
    "                \"What Is Learning?\": {\n",
    "                    \"page_start\": 182,\n",
    "                    \"page_end\": 182\n",
    "                },\n",
    "                \"Classical Conditioning\": {\n",
    "                    \"page_start\": 183,\n",
    "                    \"page_end\": 191\n",
    "                },\n",
    "                \"Operant Conditioning\": {\n",
    "                    \"page_start\": 192,\n",
    "                    \"page_end\": 202\n",
    "                },\n",
    "                \"Observational Learning (Modeling)\": {\n",
    "                    \"page_start\": 203,\n",
    "                    \"page_end\": 206\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Thinking and Intelligence\": {\n",
    "            \"page_start\": 213,\n",
    "            \"page_end\": 246,\n",
    "            \"subsections\": {\n",
    "                \"What Is Cognition?\": {\n",
    "                    \"page_start\": 214,\n",
    "                    \"page_end\": 217\n",
    "                },\n",
    "                \"Language\": {\n",
    "                    \"page_start\": 218,\n",
    "                    \"page_end\": 221\n",
    "                },\n",
    "                \"Problem Solving\": {\n",
    "                    \"page_start\": 222,\n",
    "                    \"page_end\": 227\n",
    "                },\n",
    "                \"What Are Intelligence and Creativity?\": {\n",
    "                    \"page_start\": 228,\n",
    "                    \"page_end\": 230\n",
    "                },\n",
    "                \"Measures of Intelligence\": {\n",
    "                    \"page_start\": 231,\n",
    "                    \"page_end\": 236\n",
    "                },\n",
    "                \"The Source of Intelligence\": {\n",
    "                    \"page_start\": 237,\n",
    "                    \"page_end\": 240\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Memory\": {\n",
    "            \"page_start\": 247,\n",
    "            \"page_end\": 277,\n",
    "            \"subsections\": {\n",
    "                \"How Memory Functions\": {\n",
    "                    \"page_start\": 248,\n",
    "                    \"page_end\": 254\n",
    "                },\n",
    "                \"Parts of the Brain Involved with Memory\": {\n",
    "                    \"page_start\": 255,\n",
    "                    \"page_end\": 258\n",
    "                },\n",
    "                \"Problems with Memory\": {\n",
    "                    \"page_start\": 259,\n",
    "                    \"page_end\": 268\n",
    "                },\n",
    "                \"Ways to Enhance Memory\": {\n",
    "                    \"page_start\": 269,\n",
    "                    \"page_end\": 272\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Lifespan Development\": {\n",
    "            \"page_start\": 279,\n",
    "            \"page_end\": 320,\n",
    "            \"subsections\": {\n",
    "                \"What Is Lifespan Development?\": {\n",
    "                    \"page_start\": 280,\n",
    "                    \"page_end\": 283\n",
    "                },\n",
    "                \"Lifespan Theories\": {\n",
    "                    \"page_start\": 284,\n",
    "                    \"page_end\": 291\n",
    "                },\n",
    "                \"Stages of Development\": {\n",
    "                    \"page_start\": 292,\n",
    "                    \"page_end\": 312\n",
    "                },\n",
    "                \"Death and Dying\": {\n",
    "                    \"page_start\": 313,\n",
    "                    \"page_end\": 314\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Emotion and Motivation\": {\n",
    "            \"page_start\": 321,\n",
    "            \"page_end\": 357,\n",
    "            \"subsections\": {\n",
    "                \"Motivation\": {\n",
    "                    \"page_start\": 322,\n",
    "                    \"page_end\": 327\n",
    "                },\n",
    "                \"Hunger and Eating\": {\n",
    "                    \"page_start\": 328,\n",
    "                    \"page_end\": 333\n",
    "                },\n",
    "                \"Sexual Behavior\": {\n",
    "                    \"page_start\": 334,\n",
    "                    \"page_end\": 341\n",
    "                },\n",
    "                \"Emotion\": {\n",
    "                    \"page_start\": 342,\n",
    "                    \"page_end\": 352\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Personality\": {\n",
    "            \"page_start\": 359,\n",
    "            \"page_end\": 396,\n",
    "            \"subsections\": {\n",
    "                \"What Is Personality?\": {\n",
    "                    \"page_start\": 360,\n",
    "                    \"page_end\": 361\n",
    "                },\n",
    "                \"Freud and the Psychodynamic Perspective\": {\n",
    "                    \"page_start\": 362,\n",
    "                    \"page_end\": 367\n",
    "                },\n",
    "                \"Neo-Freudians: Adler, Erikson, Jung, and Horney\": {\n",
    "                    \"page_start\": 368,\n",
    "                    \"page_end\": 372\n",
    "                },\n",
    "                \"Learning Approaches\": {\n",
    "                    \"page_start\": 373,\n",
    "                    \"page_end\": 376\n",
    "                },\n",
    "                \"Humanistic Approaches\": {\n",
    "                    \"page_start\": 377,\n",
    "                    \"page_end\": 377\n",
    "                },\n",
    "                \"Biological Approaches\": {\n",
    "                    \"page_start\": 378,\n",
    "                    \"page_end\": 378\n",
    "                },\n",
    "                \"Trait Theorists\": {\n",
    "                    \"page_start\": 379,\n",
    "                    \"page_end\": 383\n",
    "                },\n",
    "                \"Cultural Understandings of Personality\": {\n",
    "                    \"page_start\": 384,\n",
    "                    \"page_end\": 385\n",
    "                },\n",
    "                \"Personality Assessment\": {\n",
    "                    \"page_start\": 386,\n",
    "                    \"page_end\": 390\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Social Psychology\": {\n",
    "            \"page_start\": 399,\n",
    "            \"page_end\": 445,\n",
    "            \"subsections\": {\n",
    "                \"What Is Social Psychology?\": {\n",
    "                    \"page_start\": 400,\n",
    "                    \"page_end\": 405\n",
    "                },\n",
    "                \"Self-presentation\": {\n",
    "                    \"page_start\": 406,\n",
    "                    \"page_end\": 408\n",
    "                },\n",
    "                \"Attitudes and Persuasion\": {\n",
    "                    \"page_start\": 409,\n",
    "                    \"page_end\": 414\n",
    "                },\n",
    "                \"Conformity, Compliance, and Obedience\": {\n",
    "                    \"page_start\": 415,\n",
    "                    \"page_end\": 421\n",
    "                },\n",
    "                \"Prejudice and Discrimination\": {\n",
    "                    \"page_start\": 422,\n",
    "                    \"page_end\": 428\n",
    "                },\n",
    "                \"Aggression\": {\n",
    "                    \"page_start\": 429,\n",
    "                    \"page_end\": 431\n",
    "                },\n",
    "                \"Prosocial Behavior\": {\n",
    "                    \"page_start\": 432,\n",
    "                    \"page_end\": 436\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Industrial-Organizational Psychology\": {\n",
    "            \"page_start\": 447,\n",
    "            \"page_end\": 483,\n",
    "            \"subsections\": {\n",
    "                \"What Is Industrial and Organizational Psychology?\": {\n",
    "                    \"page_start\": 448,\n",
    "                    \"page_end\": 455\n",
    "                },\n",
    "                \"Industrial Psychology: Selecting and Evaluating Employees\": {\n",
    "                    \"page_start\": 456,\n",
    "                    \"page_end\": 466\n",
    "                },\n",
    "                \"Organizational Psychology: The Social Dimension of Work\": {\n",
    "                    \"page_start\": 467,\n",
    "                    \"page_end\": 476\n",
    "                },\n",
    "                \"Human Factors Psychology and Workplace Design\": {\n",
    "                    \"page_start\": 477,\n",
    "                    \"page_end\": 479\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Stress, Lifestyle, and Health\": {\n",
    "            \"page_start\": 485,\n",
    "            \"page_end\": 535,\n",
    "            \"subsections\": {\n",
    "                \"What Is Stress?\": {\n",
    "                    \"page_start\": 486,\n",
    "                    \"page_end\": 495\n",
    "                },\n",
    "                \"Stressors\": {\n",
    "                    \"page_start\": 496,\n",
    "                    \"page_end\": 501\n",
    "                },\n",
    "                \"Stress and Illness\": {\n",
    "                    \"page_start\": 502,\n",
    "                    \"page_end\": 513\n",
    "                },\n",
    "                \"Regulation of Stress\": {\n",
    "                    \"page_start\": 514,\n",
    "                    \"page_end\": 520\n",
    "                },\n",
    "                \"The Pursuit of Happiness\": {\n",
    "                    \"page_start\": 521,\n",
    "                    \"page_end\": 528\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Psychological Disorders\": {\n",
    "            \"page_start\": 537,\n",
    "            \"page_end\": 597,\n",
    "            \"subsections\": {\n",
    "                \"What Are Psychological Disorders?\": {\n",
    "                    \"page_start\": 538,\n",
    "                    \"page_end\": 541\n",
    "                },\n",
    "                \"Diagnosing and Classifying Psychological Disorders\": {\n",
    "                    \"page_start\": 542,\n",
    "                    \"page_end\": 544\n",
    "                },\n",
    "                \"Perspectives on Psychological Disorders\": {\n",
    "                    \"page_start\": 545,\n",
    "                    \"page_end\": 547\n",
    "                },\n",
    "                \"Anxiety Disorders\": {\n",
    "                    \"page_start\": 548,\n",
    "                    \"page_end\": 553\n",
    "                },\n",
    "                \"Obsessive-Compulsive and Related Disorders\": {\n",
    "                    \"page_start\": 554,\n",
    "                    \"page_end\": 557\n",
    "                },\n",
    "                \"Posttraumatic Stress Disorder\": {\n",
    "                    \"page_start\": 558,\n",
    "                    \"page_end\": 559\n",
    "                },\n",
    "                \"Mood and Related Disorders\": {\n",
    "                    \"page_start\": 560,\n",
    "                    \"page_end\": 569\n",
    "                },\n",
    "                \"Schizophrenia\": {\n",
    "                    \"page_start\": 570,\n",
    "                    \"page_end\": 573\n",
    "                },\n",
    "                \"Dissociative Disorders\": {\n",
    "                    \"page_start\": 574,\n",
    "                    \"page_end\": 575\n",
    "                },\n",
    "                \"Disorders in Childhood\": {\n",
    "                    \"page_start\": 576,\n",
    "                    \"page_end\": 581\n",
    "                },\n",
    "                \"Personality Disorders\": {\n",
    "                    \"page_start\": 582,\n",
    "                    \"page_end\": 588\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Therapy and Treatment\": {\n",
    "            \"page_start\": 599,\n",
    "            \"page_end\": 631,\n",
    "            \"subsections\": {\n",
    "                \"Mental Health Treatment: Past and Present\": {\n",
    "                    \"page_start\": 600,\n",
    "                    \"page_end\": 604\n",
    "                },\n",
    "                \"Types of Treatment\": {\n",
    "                    \"page_start\": 605,\n",
    "                    \"page_end\": 616\n",
    "                },\n",
    "                \"Treatment Modalities\": {\n",
    "                    \"page_start\": 617,\n",
    "                    \"page_end\": 620\n",
    "                },\n",
    "                \"Substance-Related and Addictive Disorders: A Special Case\": {\n",
    "                    \"page_start\": 621,\n",
    "                    \"page_end\": 622\n",
    "                },\n",
    "                \"The Sociocultural Model and Therapy Utilization\": {\n",
    "                    \"page_start\": 623,\n",
    "                    \"page_end\": 626\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "create_sections_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "error",
     "timestamp": 1737557404395,
     "user": {
      "displayName": "chaimae hadrouch",
      "userId": "11701385554007709212"
     },
     "user_tz": -60
    },
    "id": "ABG0voBJct6e",
    "outputId": "b41fe15f-7122-467e-c617-851e3eb3407f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2706e7076600>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- HybridRetriever Class ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mHybridRetriever\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_model_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all-mpnet-base-v2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_encoder_model_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cross-encoder/ms-marco-MiniLM-L-12-v2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \"\"\"\n\u001b[1;32m      5\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mhybrid\u001b[0m \u001b[0mretriever\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcross\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mranking\u001b[0m \u001b[0mcapabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2706e7076600>\u001b[0m in \u001b[0;36mHybridRetriever\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbm25\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBM25Okapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mfine_tune_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_examples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInputExample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[1;32m     57\u001b[0m         \u001b[0mFine\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtune\u001b[0m \u001b[0mSentenceTransformer\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0musing\u001b[0m \u001b[0mdomain\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mspecific\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "# --- HybridRetriever Class ---\n",
    "class HybridRetriever:\n",
    "    def __init__(self, book_data, embedding_model_name='all-mpnet-base-v2', cross_encoder_model_name='cross-encoder/ms-marco-MiniLM-L-12-v2', adapter=None, dense_weight=0.5):\n",
    "        \"\"\"\n",
    "        Initialize hybrid retriever with dense, sparse, and cross-encoder re-ranking capabilities.\n",
    "\n",
    "        Args:\n",
    "            book_data (List[Dict]): List of book sections\n",
    "            embedding_model_name (str): Sentence transformer model name for dense retrieval\n",
    "            cross_encoder_model_name (str): Cross-encoder model name for re-ranking\n",
    "            adapter (str): Path to adapter model or None\n",
    "            dense_weight (float): Weight for dense semantic search (0-1)\n",
    "        \"\"\"\n",
    "        # Device configuration\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.dense_weight = dense_weight\n",
    "\n",
    "        # Load embedding model with optional adapter\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name).to(self.device)\n",
    "        if adapter:\n",
    "            print(f\"Loading adapter from {adapter}...\")\n",
    "            self.embedding_model.load_state_dict(torch.load(adapter))\n",
    "\n",
    "        # Cross-encoder model for re-ranking\n",
    "        self.cross_encoder_model = CrossEncoder(cross_encoder_model_name)\n",
    "\n",
    "        # Prepare book data\n",
    "        self.book_data = book_data\n",
    "        self.texts = [item['text'] for item in book_data]\n",
    "\n",
    "        # Dense retrieval (FAISS index)\n",
    "        self._create_dense_index()\n",
    "\n",
    "        # Sparse retrieval (BM25)\n",
    "        self._create_bm25_index()\n",
    "\n",
    "    def _create_dense_index(self):\n",
    "        \"\"\"Create FAISS index for dense semantic search\"\"\"\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            self.texts,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True\n",
    "        ).cpu().numpy()\n",
    "\n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.faiss_index = faiss.IndexFlatIP(self.dimension)\n",
    "        self.faiss_index.add(embeddings)\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def _create_bm25_index(self):\n",
    "        \"\"\"Create BM25 index for sparse keyword-based retrieval\"\"\"\n",
    "        self.tokenized_texts = [text.lower().split() for text in self.texts]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_texts)\n",
    "\n",
    "    def fine_tune_embeddings(self, train_examples: List[InputExample], epochs=3, batch_size=8, learning_rate=3e-5):\n",
    "        \"\"\"\n",
    "        Fine-tune SentenceTransformer model using domain-specific data.\n",
    "\n",
    "        Args:\n",
    "            train_examples (List[InputExample]): Training data\n",
    "            epochs (int): Number of fine-tuning epochs\n",
    "            batch_size (int): Batch size\n",
    "            learning_rate (float): Learning rate for optimization\n",
    "        \"\"\"\n",
    "        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "        train_loss = losses.MultipleNegativesRankingLoss(self.embedding_model)\n",
    "\n",
    "        self.embedding_model.fit(\n",
    "            train_objectives=[(train_dataloader, train_loss)],\n",
    "            epochs=epochs,\n",
    "            warmup_steps=100,\n",
    "            optimizer_params={'lr': learning_rate},\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "\n",
    "    def save_adapter(self, save_path):\n",
    "        \"\"\"\n",
    "        Save the fine-tuned embedding adapter.\n",
    "\n",
    "        Args:\n",
    "            save_path (str): Path to save the adapter\n",
    "        \"\"\"\n",
    "        print(f\"Saving adapter to {save_path}...\")\n",
    "        torch.save(self.embedding_model.state_dict(), save_path)\n",
    "\n",
    "    def retrieve_sections(self, query: str, k_initial: int = 10, k_final: int = 3, window_size=2):\n",
    "        \"\"\"\n",
    "        Hybrid retrieval combining dense and sparse search with cross-encoder re-ranking\n",
    "\n",
    "        Args:\n",
    "            query (str): Search query\n",
    "            k_initial (int): Number of top sections to retrieve initially\n",
    "            k_final (int): Number of top sections to return after re-ranking\n",
    "            window_size (int): Window size for context enrichment (not used in this version, but could be added later)\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            List of top k_final retrieved sections with re-ranked scoring\n",
    "        \"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query], convert_to_tensor=True).cpu().numpy()\n",
    "        dense_distances, dense_indices = self.faiss_index.search(query_embedding, k_initial)\n",
    "\n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "\n",
    "\n",
    "        hybrid_scores = []\n",
    "        for i in range(len(self.texts)):\n",
    "            dense_score = 0\n",
    "            if i in dense_indices[0]:\n",
    "                dense_idx = np.where(dense_indices[0] == i)[0]\n",
    "                if len(dense_idx) > 0:\n",
    "                    dense_score = dense_distances[0][dense_idx[0]]\n",
    "            sparse_score = bm25_scores[i]\n",
    "            hybrid_score = (self.dense_weight * dense_score) + ((1 - self.dense_weight) * sparse_score)\n",
    "            hybrid_scores.append((hybrid_score, i))\n",
    "\n",
    "        hybrid_scores.sort(reverse=True, key=lambda x: x[0])\n",
    "        top_k_initial_indices = [idx for _, idx in hybrid_scores[:k_initial]]\n",
    "\n",
    "        cross_encoder_pairs = [[query, self.texts[idx]] for idx in top_k_initial_indices]\n",
    "        cross_encoder_scores = self.cross_encoder_model.predict(cross_encoder_pairs)\n",
    "\n",
    "        final_scores = []\n",
    "        for i, idx in enumerate(top_k_initial_indices):\n",
    "            initial_score = hybrid_scores[i][0]\n",
    "            re_ranked_score = cross_encoder_scores[i]\n",
    "            combined_score = 0.6 * initial_score + 0.4 * re_ranked_score\n",
    "            final_scores.append((combined_score, idx))\n",
    "\n",
    "        final_scores.sort(reverse=True, key=lambda x: x[0])\n",
    "        top_k_final_indices = [idx for _, idx in final_scores[:k_final]]\n",
    "\n",
    "        results = [\n",
    "            {\n",
    "                \"score\": final_scores[i][0],\n",
    "                \"page\": self.book_data[idx]['page'],\n",
    "                \"text\": self.book_data[idx]['text']\n",
    "            }\n",
    "            for i, idx in enumerate(top_k_final_indices)\n",
    "        ]\n",
    "\n",
    "        return results\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def load_book(pdf_path, sections_metadata_path=\"/content/drive/MyDrive/RAG_Project/sections.json\"):\n",
    "    \"\"\"Load and preprocess the book from PDF, including section metadata.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    sections_data = []\n",
    "\n",
    "    # Load section metadata\n",
    "    with open(sections_metadata_path, \"r\") as f:\n",
    "        sections_metadata = json.load(f)\n",
    "\n",
    "    # Create a flattened page-to-section mapping for easier lookup\n",
    "    page_to_section_map = {}\n",
    "\n",
    "    def map_sections(section_name, section_data, parent_path=\"\"):\n",
    "        current_path = f\"{parent_path}/{section_name}\" if parent_path else section_name\n",
    "        for page_num in range(section_data[\"page_start\"], section_data[\"page_end\"] + 1):\n",
    "            page_to_section_map[page_num] = current_path\n",
    "\n",
    "        if \"subsections\" in section_data:\n",
    "            for subsection_name, subsection_data in section_data[\"subsections\"].items():\n",
    "                map_sections(subsection_name, subsection_data, current_path)\n",
    "\n",
    "    for section_name, section_data in sections_metadata.items():\n",
    "        map_sections(section_name, section_data)\n",
    "\n",
    "    for page_num, page in enumerate(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        if text.strip():\n",
    "            section_path = page_to_section_map.get(page_num + 1, \"Unknown\")\n",
    "            sections_data.append({\"page\": page_num + 1, \"text\": text.strip(), \"section\": section_path})\n",
    "\n",
    "    return sections_data, page_to_section_map\n",
    "\n",
    "def generate_answer(context, question, device):\n",
    "    \"\"\"Generate an answer using the generative model.\"\"\"\n",
    "    generator_model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "    generator_tokenizer = AutoTokenizer.from_pretrained(generator_model_name)\n",
    "    generator_model = AutoModelForCausalLM.from_pretrained(generator_model_name).to(device)\n",
    "\n",
    "    # Refine the prompt\n",
    "    input_text = f\"\"\"\n",
    "    Answer VERY concisely from the provided context only. Your answer length: less or equal 150 tokens. Plain text, no Markdown. Answer only, no other questions.\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Tokenize with padding and attention mask\n",
    "        inputs = generator_tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,  # Enable padding\n",
    "            truncation=True,\n",
    "            max_length=512  # Adjust max length as needed\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Set pad_token_id if not already set\n",
    "        if generator_tokenizer.pad_token_id is None:\n",
    "            generator_tokenizer.pad_token_id = generator_tokenizer.eos_token_id\n",
    "\n",
    "        # Generate answer with attention mask and pad_token_id\n",
    "        output_ids = generator_model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=generator_tokenizer.pad_token_id,\n",
    "            num_return_sequences=1,\n",
    "            #top_p=0.7,\n",
    "            #top_k=20,\n",
    "            num_beams=5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            #temperature=0.1,\n",
    "            max_new_tokens=512\n",
    "        )\n",
    "        return generator_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in answer generation: {e}\")\n",
    "        return \"Unable to generate an answer.\"\n",
    "\n",
    "def clean_answer(answer):\n",
    "    \"\"\"Cleans up the generated answer with more targeted operations.\"\"\"\n",
    "\n",
    "    # 1. Remove excessive newlines (but keep single newlines)\n",
    "    answer = answer.replace(\"\\n\\n\\n\", \"\\n\\n\").replace(\"\\n\\n###\", \"###\").replace(\"\\n\\n-\", \"-\")\n",
    "\n",
    "    # 2. Handle LaTeX-like formatting VERY selectively (example):\n",
    "    answer = re.sub(r'\\\\\\(.*?\\\\\\)', '', answer)  # Remove only \\( ... \\)\n",
    "    answer = re.sub(r'\\\\\\[.*?\\\\\\]', '', answer)  # Remove only \\[ ... \\]\n",
    "\n",
    "    # 3. Remove stray backslashes and unnecessary commands\n",
    "    answer = re.sub(r'\\\\(text|quad|boxed|textit|textbf){', '', answer)\n",
    "    answer = re.sub(r'(?<!\\\\)}', '}', answer)  # Remove } that is not preceded by a backslash\n",
    "    answer = re.sub(r'(?![a-zA-Z])\\\\', '', answer)  # Remove \\ followed by non-letters (likely stray)\n",
    "    answer = re.sub(r'\\\\\\$', '$', answer)  # Replace \\$ with $\n",
    "    answer = re.sub(r'\\\\%', '%', answer)  # Replace \\% with %\n",
    "\n",
    "    # 4. Remove Markdown bold formatting (**)\n",
    "    answer = answer.replace(\"**\", \"\")\n",
    "\n",
    "    # 5. Remove multiple spaces, but keep single spaces\n",
    "    answer = ' '.join(answer.split())\n",
    "\n",
    "    return answer\n",
    "\n",
    "def load_queries(json_path):\n",
    "    \"\"\"Load questions from the queries JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            queries = json.load(f)\n",
    "        return queries\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading queries: {e}\")\n",
    "        return []\n",
    "\n",
    "def rag_pipeline(question, hybrid_retriever, device, page_to_section_map, k=3, window_size=2):\n",
    "    \"\"\"\n",
    "    Updated RAG pipeline using hybrid retrieval with context window expansion.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to answer.\n",
    "        hybrid_retriever (HybridRetriever): The hybrid retriever object.\n",
    "        device (torch.device): The device to use for computation (CPU or CUDA).\n",
    "        page_to_section_map (dict): Mapping of page numbers to section paths.\n",
    "        k (int): Number of top sections to retrieve after re-ranking.\n",
    "        window_size (int): Number of pages to include on either side of the retrieved page.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the context, answer, and references.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        retrieved_pages = hybrid_retriever.retrieve_sections(question, k_initial=10, k_final=k)  # Increased k_initial\n",
    "\n",
    "        if not retrieved_pages:\n",
    "            return {\n",
    "                \"context\": \"\",\n",
    "                \"answer\": \"No relevant context found.\",\n",
    "                \"references\": json.dumps({\"sections\": [], \"pages\": []})\n",
    "            }\n",
    "\n",
    "        # Group pages by section path\n",
    "        section_page_mapping = {}\n",
    "        for page_data in retrieved_pages:\n",
    "            page_num = page_data['page']\n",
    "            section_path = page_to_section_map.get(page_num, \"Unknown\")\n",
    "            if section_path not in section_page_mapping:\n",
    "                section_page_mapping[section_path] = []\n",
    "            section_page_mapping[section_path].append(page_num)\n",
    "\n",
    "        # Sort pages within each section\n",
    "        for section_path, pages in section_page_mapping.items():\n",
    "            section_page_mapping[section_path] = sorted(pages)\n",
    "\n",
    "        # Construct context with window_size\n",
    "        context_pages = []\n",
    "        num_pages = len(page_to_section_map)\n",
    "        for section_path, pages in section_page_mapping.items():\n",
    "            for page in pages:\n",
    "                # Expand context to include pages within the window\n",
    "                for i in range(max(1, page - window_size), min(num_pages, page + window_size + 1)):\n",
    "                    context_pages.append(i)\n",
    "        context_pages = sorted(list(set(context_pages)))  # Remove duplicates and sort\n",
    "\n",
    "        # Retrieve the text for the context pages\n",
    "        context = \"\"\n",
    "        for page_num in context_pages:\n",
    "            page_text = [p['text'] for p in hybrid_retriever.book_data if p['page'] == page_num]\n",
    "            if page_text:\n",
    "                context += page_text[0] + \"\\n\"\n",
    "\n",
    "        # Generate the answer using the context and question\n",
    "        answer = generate_answer(context, question, device)\n",
    "\n",
    "        # Clean the generated answer\n",
    "        cleaned_answer = clean_answer(answer)\n",
    "\n",
    "        references = {\n",
    "            \"sections\": list(section_page_mapping.keys()),\n",
    "            \"pages\": [str(page) for pages in section_page_mapping.values() for page in pages]\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"context\": context,\n",
    "            \"answer\": cleaned_answer,\n",
    "            \"references\": json.dumps(references)\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in RAG pipeline: {e}\")\n",
    "        return {\n",
    "            \"context\": \"\",\n",
    "            \"answer\": \"An error occurred during processing.\",\n",
    "            \"references\": json.dumps({\"sections\": [], \"pages\": []})\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Define the dataset directory (Kaggle input)\n",
    "    dataset_dir = \"/content/drive/MyDrive/RAG_Project/\"\n",
    "    pdf_path = os.path.join(dataset_dir, \"book.pdf\")\n",
    "    queries_path = os.path.join(dataset_dir, \"queries.json\")\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the book along with section metadata\n",
    "    book_data, page_to_section_map = load_book(pdf_path)\n",
    "\n",
    "    # Initialize hybrid retriever\n",
    "    hybrid_retriever = HybridRetriever(book_data)\n",
    "\n",
    "    # Load queries\n",
    "    queries = load_queries(queries_path)\n",
    "\n",
    "    # Generate submission data\n",
    "    submission_data = []\n",
    "    for query in queries:\n",
    "        query_id = query.get(\"query_id\", \"unknown\")\n",
    "        question = query.get(\"question\", \"\")\n",
    "\n",
    "        if not question:\n",
    "            print(f\"Skipping query with ID {query_id} because the question is empty.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = rag_pipeline(question, hybrid_retriever, device, page_to_section_map, window_size=2)\n",
    "            print(f\"Response for query ID {query_id}: {response}\")\n",
    "\n",
    "            submission_data.append({\n",
    "                \"ID\": query_id,\n",
    "                \"context\": response.get(\"context\", \"\"),\n",
    "                \"answer\": response.get(\"answer\", \"\"),\n",
    "                \"references\": response.get(\"references\", \"{}\")\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query {query_id}: {e}\")\n",
    "            submission_data.append({\n",
    "                \"ID\": query_id,\n",
    "                \"context\": \"\",\n",
    "                \"answer\": \"Error processing query\",\n",
    "                \"references\": \"{}\"\n",
    "            })\n",
    "\n",
    "    # Create submission CSV\n",
    "    submission_df = pd.DataFrame(submission_data)\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)  # Save to /content/drive/MyDrive/RAG_Project/\n",
    "\n",
    "    print(\"Submission file 'submission.csv' has been generated successfully!\")\n",
    "    print(\"\\nSubmission Statistics:\")\n",
    "    print(f\"Total rows: {len(submission_df)}\")\n",
    "    print(f\"Average context length: {submission_df['context'].str.len().mean():.0f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPjxflBT0YvJhGrnTaO0Ozd",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
